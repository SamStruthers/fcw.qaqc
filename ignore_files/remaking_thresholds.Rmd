---
title: "Remaking Seasonal Thresholds with Manually Verified 2023 Data"
author: "ROSSyndicate/Juan De La Torre"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  markdown: 
    wrap: 90
---

```{r}
# Loading libraries
library(tidyverse)
library(here)
library(data.table)
library(lubridate)
library(plotly)
library(ggplot2)
library(arrow)
library(yaml)

# Pull in the functions as they are in synapse
walk(list.files(here("R"), pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
options(arrow.unsafe_metadata = TRUE)
```

# Defining Thresholds
The definition for these thresholds (as defined by `make_threshold_table`) is:
- `slope_down`: the 99th quantile of the negative `slope_behind` values, grouped by season 
- `slope_up`: the 1st quantile of the positive `slope_behind` values, grouped by season 
- `f01`: the 99th quantile of the mean, grouped by season
- `f99`: the 1st quantile of the mean, grouped by season

These definitions require clean data to accurately generate the thresholds.
For this reason we will generate them using the manually verified 2023 data.
When the 2024 data is manually verified, we will use that data to update 
these thresholds

```{r Generating all possible combinations}
# Setting levels and creating all possible site-parameter-season combinations
# for threshold analysis.

site_levels <- c(
  "bellvue",
  "salyer",
  "udall",
  "riverbend",
  "cottonwood",
  "elc",
  "archery",
  "riverbluffs"
)

old_sites <-c(
  "tamasag",    
  "legacy",     
  "lincoln",    
  "timberline", 
  "prospect",   
  "boxelder",   
  "archery",    
  "river bluffs"
) 

parameter_levels <- c(
  "Chl-a Fluorescence",
  "Depth",
  "DO",
  "ORP",
  "pH",
  "Specific Conductivity",
  "Temperature",
  "Turbidity"
) 

season_levels <- c(
  "winter_baseflow",
  "snowmelt",
  "monsoon",
  "fall_baseflow"
)

all_combinations <- crossing(
  site = site_levels,
  parameter = parameter_levels,
  season = season_levels
)
```

## Read in the thresholds we used to flag the 2025 data
This data has the 2023 data to _some_ point in 2024
```{r Read in the old thresholds}
old_thresholds <- read_csv(here("test_data", "qaqc_files", "seasonal_thresholds.csv"), show_col_types = FALSE) %>%
  filter(parameter %in% parameter_levels) %>%
  select(season, site, parameter, old_t_mean01 = t_mean01, old_t_mean99 = t_mean99)
```

## Read in the manual thresholds we generated via field experience
```{r Read in the manual thresholds}
manual_thresholds <- read_csv(here("..", "poudre_sonde_network", "data", "qaqc", "realistic_thresholds.csv"), show_col_types = FALSE) %>%
  filter(parameter %in% parameter_levels)
```

## Read in the data to generate the new thresholds
### Read in the verified 2023 data
This is the 2023 verified data filtered for seasonal threshold analysis
```{r Read in the manually verified 2023 data}
# Pulling in the 2023 data
pwqn_dir_path <- here("..", "poudre_sonde_network")

# Update this to the post verified data
verified_data_files <- list.files(here(pwqn_dir_path, "data", "virridy_verification", "verified_directory"),
                                  full.names = TRUE
)

manual_verified_2023 <- verified_data_files %>%
  map_dfr(function(file_path) {
    verified_df <- read_rds(file_path) %>%
      mutate(verification_status = as.character(verification_status))
  }) %>%
  data.table() %>%
  filter(!grepl("virridy", site, ignore.case = TRUE)) %>%
  mutate(
    site = case_when(
      site == "tamasag" ~ "bellvue",
      site == "legacy" ~ "salyer",
      site == "lincoln" ~ "udall",
      site == "timberline" ~ "riverbend",
      site == "prospect" ~ "cottonwood",
      site == "boxelder" ~ "elc",
      site == "archery" ~ "archery",
      site == "river bluffs" ~ "riverbluffs",
      TRUE ~ site 
    ),
    clean_mean = case_when(
      is.na(flag) & verification_status == "PASS" ~ mean,
      is.na(flag) & verification_status == "FAIL" ~ NA,
      !is.na(flag) & verification_status == "PASS" ~ NA,
      !is.na(flag) & verification_status == "FAIL" ~ mean
    ),
    clean_flag = case_when(
      is.na(flag) & verification_status == "PASS" ~ NA,
      is.na(flag) & verification_status == "FAIL" ~ "MANUAL FLAG",
      !is.na(flag) & verification_status == "PASS" ~ flag,
      !is.na(flag) & verification_status == "FAIL" ~ NA
    )
  ) %>%
  filter(
    !is.na(site),
    site %in% site_levels,
    parameter %in% parameter_levels,
    DT_round <= as.POSIXct("2023-12-31 11:59:59", tz = "MST")
  ) %>%
  select(DT_round, DT_join, site, parameter, mean = clean_mean, flag = clean_flag) %>%
  split(f = list(.$site, .$parameter), sep = "-")

# Generate the summary statistics for the 2023 verified data
summary_stats_2023 <- manual_verified_2023  %>%
  map(~generate_summary_statistics(.))
```

### Read in the 2024 data 
This data is not manually verified and we are going to use the `good-ish` data cleaning
method for this data

We basically have to run through the whole pipeline for 2024 data to be summarized,
except that during the flagging step we do the same flags that were done in 
for_azure.Rmd with some alterations to match the new functions
```{r}
# Preemptively make a complete site list and start/end dates. We will use these to filter
complete_site_list <- c("bellvue", "salyer", "udall", "riverbend", 
                        "cottonwood", "elc", "archery", "riverbluffs", 
                        "tamasag", "legacy", "lincoln", "timberline", 
                        "prospect", "boxelder", "river bluffs")
complete_site_str <- paste0(complete_site_list, collapse = "|")

start_dt = as.POSIXct("2024-01-01 00:00:00", tz = "America/Denver")
start_dt = with_tz(start_dt, tzone = "UTC")

end_dt = as.POSIXct("2024-12-31 11:59:59", tz = "America/Denver")
end_dt = with_tz(end_dt, tzone = "UTC")

# make a site fixing function to save space and increase readability
fix_sites <- function(df) {
  fixed_df <- df %>% 
    filter(site %in% complete_site_list) %>% 
    # renaming all the sites, just in case
    mutate(site = case_when(
      site == "tamasag" ~ "bellvue",
      site == "legacy" ~ "salyer",
      site == "lincoln" ~ "udall",
      site == "timberline" ~ "riverbend",
      site == "prospect" ~ "cottonwood",
      site == "boxelder" ~ "elc",
      site == "archery" ~ "archery",
      site == "river bluffs" ~ "riverbluffs",
      TRUE ~ site)
    )
  return(fixed_df)
}
```

```{r Read in the 2024 data}
# Load field and malfunction notes
mWater_data <- load_mWater(creds = read_yaml(here("creds", "mWaterCreds.yml")))
# Make a filter for both all_field_notes and sensor_malfunction_notes
all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data)  %>% 
  filter(field_season == 2024) %>% 
  fix_sites()
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) %>% 
  filter(start_DT >= start_dt & start_DT <= end_dt,
         end_DT >= start_dt & end_DT <= end_dt) %>% 
  fix_sites()

# Load the HydroVu data into a file locally
## Create a custom api_start_dates for the 2024 data
api_start_dates <- tibble(site = complete_site_list,
                          start_DT = start_dt,
                          end_DT = end_dt)

## Configure HydroVu API call
hv_creds <- read_yaml(here("creds", "HydroVuCreds.yml"))
hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))
hv_sites <- hv_locations_all(hv_token) %>% 
  # filter out vulink and virridy data
  filter(!grepl("vulink|virridy", name, ignore.case = TRUE),
         # filter for the sites we are interested in
         grepl(complete_site_str, name, ignore.case = TRUE))

## Load the HydroVu data
pwalk(api_start_dates,
      function(site, start_DT, end_DT) {
        message("Requesting HydroVu data for site: ", site)
        api_puller(
          site = site,
          start_dt = start_DT,
          end_dt = end_DT,
          api_token = hv_token,
          hv_sites_arg = hv_sites,
          dump_dir = here("ignore_files", "data", "2024_data"),
          synapse_env = FALSE,
          fs = NULL
        )
      }
)

# Munge the data
## Since the data is a lil weird, we are going to use the new munge api data 
## function, but do some follow up clean up for this instance
new_data_2024 <- munge_api_data(api_dir = here("ignore_files", "data", "2024_data")) %>% 
  # Clean up
  fix_sites() %>% 
  # Split
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::keep(~ nrow(.) > 0)

# Format and Summarize the data
sites <- unique(dplyr::bind_rows(new_data_2024) %>% dplyr::pull(site))
params <- c("Chl-a Fluorescence", "Depth", "DO", "ORP", "pH", "Specific Conductivity",
            "Temperature", "Turbidity")
site_param_combos <- tidyr::crossing(sites, params) %>% 
  dplyr::mutate(combo = paste0(sites, "-", params)) %>% 
  dplyr::pull(combo) 

## Make a list of the 15-minute summarized data, joined with field notes
new_data_2024_subset <- new_data_2024[names(new_data_2024) %in% site_param_combos]

new_data_2024_tidy <- map(new_data_2024_subset,
                          function(new_data_df) {
                            tidy_api_data(
                              api_data = new_data_df,
                              summarize_interval = "15 minutes"
                            )
                          }) %>% 
  keep(~ !is.null(.)) %>% 
  # Since we are not using the 24 hour data prior to this we can skip the combine data step
  map(~add_field_notes(df = ., notes = all_field_notes))

# Add the threshold flags to make good-ish data
## read in the sensor spec thresholds
sensor_spec_threshold <- read_yaml(here("test_data", "qaqc_files", "sensor_spec_thresholds.yml"))

flagged_filtered_2024 <- new_data_2024_summarized %>% 
  bind_rows() %>%
  split(f = .$site, sep = "-") %>%
  map(., function(data) {
    data %>%
      data.table(.) %>% 
      ## Intrasonde Flags
      add_frozen_flag(df = .) %>%
      add_unsubmerged_flag(df = .) 
  }) %>% 
  bind_rows() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>% 
  discard(~ nrow(.) == 0) %>% 
  map(., function(data) {
    data %>% data.table(.) %>% 
      fix_turbidity(df = .) %>%
      ## Single Sonde Flags
      add_field_flag(df = .) %>%
      add_spec_flag(df = ., spec_table = sensor_spec_threshold) %>%
      add_na_flag(df = .) %>%
      add_repeat_flag(df = .) %>%
      add_depth_shift_flag(df = ., level_shift_table =  all_field_notes, post2024 = TRUE) %>% 
      add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes) %>% 
      rename(auto_flag = flag) %>% 
      tidy_flag_column(df = .) %>% 
      add_suspect_flag(df = .) %>% 
      # Filtering the data for what we care about for the thresholds
      mutate(auto_flag = ifelse(auto_flag == "", NA, auto_flag),
             mal_flag = ifelse(mal_flag == "", NA, mal_flag),
             mean = case_when(
               !is.na(auto_flag) ~ NA,
               sonde_employed == 1 ~ NA,
               !is.na(depth_change) | depth_change == "sonde moved" ~ NA,
               !is.na(mal_flag) ~ NA,
               TRUE ~ mean
             )
      ) %>% 
      select(DT_round, DT_join, site, parameter, mean, flag = auto_flag)
  })


# Add the summary stats
new_data_2024_summarized <- flagged_filtered_2024 %>% 
  map(~generate_summary_statistics(.))

# Make the 2024 goodish data thresholds
thresholds_2024 <- map_dfr(new_data_2024_summarized, make_threshold_table)
```

## Generate the new thresholds from the updated data (2023 & 2024)
First we will bind the 2023 manually verified data and the clean-ish 2024 data. 
We will use these bound data to make the new thresholds
```{r Bind 2023 and 2024}
bound_2023_2024 <- map(
  site_param_combos,
  function(idx) {
    # Get the index from the list
    old_data_2023_df <- manual_verified_2023[[idx]]
    new_data_2024_df <- flagged_filtered_2024[[idx]]
    
    old_data_unavailable <- (is.null(old_data_2023_df)||nrow(old_data_2023_df) == 0)
    new_data_unavailable <- (is.null(new_data_2024_df)||nrow(new_data_2024_df) == 0)
    
    if (new_data_unavailable && old_data_unavailable) {
      return(NULL)
    } else if (!new_data_unavailable && old_data_unavailable) {
      return(new_data_2024_df)
    } else if (new_data_unavailable && !old_data_unavailable) {
      return(old_data_2023_df)
    } else {
      bound_data <- bind_rows(old_data_2023_df, new_data_2024_df) %>% 
        distinct(DT_round, .keep_all = TRUE) %>% 
        arrange(DT_round)
      return(bound_data)
    }
  }
)
names(bound_2023_2024) <- site_param_combos 
bound_2023_2024 <- bound_2023_2024 %>% 
  compact()
```

```{r Generate the new thresholds using the bound data}
# Generate the new thresholds (using manually verified 2023 data and good-ish 2024 data)
# threshold_lookup <- purrr::map_dfr(summary_stats_2023, make_threshold_table)

summarized_bound_2023_2024 <- bound_2023_2024 %>% 
  map(~generate_summary_statistics(.)) 

bound_thresholds <- purrr::map_dfr(summarized_bound_2023_2024, make_threshold_table)

bound_thresholds_filtered <- threshold_lookup %>%
  select(season, site, parameter, new_t_mean01 = t_mean01, new_t_mean99 = t_mean99)
```
```{r}
# compare 
old
new
combined
manual
```

# Determine which thresholds should be used

```{r}
# If more than 70\% of the data was NA for a season, use the manual thresholds
threshold_validity_check <- summary_stats_2023 %>%
  map_dfr(function(site_param_df) {
    df <- site_param_df %>%
      group_by(site, parameter, season) %>%
      summarize(
        rows = n(),
        non_empty_rows = sum(!is.na(mean)) / n()
      )
  }) %>%
  mutate(needs_manual_thresholds = non_empty_rows <= 0.70) %>%
  select(site, parameter, season, needs_manual_thresholds) %>%
  filter(needs_manual_thresholds) %>%
  distinct(site, parameter, season)

# join threshold_validity_check with manual_seasonal_thresholds
manual_thresholds_required <- left_join(
  threshold_validity_check,
  manual_seasonal_thresholds,
  by = "parameter"
) %>%
  select(site, parameter, season, manual_t_mean01 = min, manual_t_mean99 = max)
manual_thresholds_required
```

## Column Definitions for holistic_thresholds
season, site, and parameter are self explanatory.
The new and old thresholds were both made using the 2023 data.
`new_t_mean(01|99)`: The thresholds that were made after the 2023 data had been verified
`old_t_mean(01|99)`: The thresholds that were made before the 2023 data had been verified
`manual_t_mean(01|99)`: The thresholds that were made via field experience
```{r}
# Replace the data that we have with the data that we got from manual thresholds required
# what do i do about the slopes?
holistic_thresholds <- left_join(threshold_comparison,
                                 manual_thresholds_required,
                                 by = c("site", "parameter", "season")
)
holistic_thresholds
```

```{r}
# we are missing all of the theoretical combinations, so we should fill those in too.
official_thresholds <- holistic_thresholds %>%
  mutate(
    official_t_mean01 = ifelse(is.na(manual_t_mean01), new_t_mean01, manual_t_mean01),
    official_t_mean99 = ifelse(is.na(manual_t_mean99), new_t_mean99, manual_t_mean99)
  ) %>%
  select(site, parameter, season, official_t_mean01, official_t_mean99)
official_thresholds
```

# lets start here really fast, and then we will re organize
```{r}
# Here are all the theoritically possible site-parameter-season combinations
all_crossings <- crossing(
  site = unique(manual_verified_2023$site),
  parameter = unique(manual_verified_2023$parameter),
  season = c("winter_baseflow", "snowmelt", "monsoon", "fall_baseflow")
)
all_crossings
# going to have to do something about the missing combos in the future
```

```{r}
new_thresholds <- threshold_lookup_filtered
old_thresholds
manual_thresholds <- manual_thresholds_required
# Left join each of these with all_crossings
all_thresholds <- all_crossings %>%
  left_join(new_thresholds, by = c("site", "parameter", "season")) %>%
  left_join(old_thresholds, by = c("site", "parameter", "season")) %>%
  left_join(manual_thresholds, by = c("site", "parameter", "season"))
all_thresholds
```

```{r}
official_thresholds <- all_thresholds %>%
  mutate(
    official_t_mean01 = ifelse(is.na(manual_t_mean01), new_t_mean01, manual_t_mean01),
    official_t_mean99 = ifelse(is.na(manual_t_mean99), new_t_mean99, manual_t_mean99)
  ) %>%
  left_join(manual_seasonal_thresholds, by = c("parameter")) %>%
  mutate(
    official_t_mean01 = ifelse(is.na(official_t_mean01), min, official_t_mean01),
    official_t_mean99 = ifelse(is.na(official_t_mean99), max, official_t_mean99)
  ) %>%
  select(site, parameter, season, official_t_mean01, official_t_mean99)
```

```{r}
# Setting functions
threshold_retriever <- function(df = official_thresholds, site_arg, param_arg) {
  season_levels <- c("winter_baseflow", "snowmelt", "monsoon", "fall_baseflow")
  filtered_df <- df %>%
    filter(
      site %in% site_arg,
      parameter %in% param_arg
    ) %>%
    mutate(season = factor(season, levels = season_levels)) %>%
    arrange(season)
  return(tibble(
    site = site_arg,
    parameter = param_arg,
    season = filtered_df$season,
    t_mean01 = filtered_df$official_t_mean01, t_mean99 = filtered_df$official_t_mean99
  ))
}
threshold_retriever(site_arg = "archery", param_arg = "Depth")
```


