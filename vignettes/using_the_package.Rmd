---
title: "Using 'fcw_qaqc'"
author: "Katie Willi"
date: "2025-02-12"
output: html_document
---

```{r, warning=FALSE, results='hide', echo=FALSE}
package_loader <- function(x) {
  if (x %in% installed.packages()) {
    suppressMessages({
      library(x, character.only = TRUE)
    })
  } else {
    suppressMessages({
      install.packages(x)
      library(x, character.only = TRUE)
    })
  }
}

invisible(
  lapply(c("data.table", "httr2", "tidyverse", 
           "rvest", "readxl", "lubridate", 
           "zoo", "padr", "stats",
           "plotly", "feather", "RcppRoll", 
           "yaml", "ggpubr", "profvis", 
           "janitor", "here"), package_loader)
)

walk(list.files('R/', pattern = "*.R", full.names = TRUE, recursive = TRUE), source)
```

### Creating our test subset of data:

```{r, eval = FALSE}
# Load in old data that has gone thru the auto-QAQC pipeline:
# making a dummy version:
historical_data <- readRDS("data/pwqn_data.RDS") %>% 
  dplyr::filter(DT_round <= "2024-11-01 01:00:00 MST") 
  
saveRDS(historical_data, "data/pwqn_data.RDS")
```

### Ensuring api directory is clear:

```{r}
if(length(list.files("data/api")) > 0) {stop("ERROR OCCURED - API DATA IN API FOLDER")}
```


##  Step 1: Import and collate data 

*Load in mWater field notes*

```{r}
mWater_data <- load_mWater(creds = yaml::read_yaml("creds/mWaterCreds.yml"))

all_field_notes <- grab_mWater_sensor_notes(mWater_api_data = mWater_data) %>%
  # lil' renaming for consistency across data sets. JD - IS THIS STILL REQUIRED?
  dplyr::mutate(site = ifelse(site == "river bluffs", "riverbluffs", site))

# pull in user-defined instances of sensor or sonde malfunction, burial, drift, etc.
sensor_malfunction_notes <- grab_mWater_malfunction_notes(mWater_api_data = mWater_data) 
```

*Load in the historically flagged data*

```{r}
historical_data <- readRDS("data/pwqn_data.RDS") %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::keep(~!is.null(.) & nrow(.) > 0)
```

*Grab new sonde data. We use the historical data to find the last time data was downloaded and use that as the start time.*

```{r api}
api_start_dates <- get_start_dates(incoming_historically_flagged_data_list = historical_data)

hv_creds <- yaml::read_yaml("creds/HydroVuCreds.yml")

hv_token <- hv_auth(client_id = as.character(hv_creds["client"]),
                    client_secret = as.character(hv_creds["secret"]))

incoming_data_csvs_upload <-  purrr::walk2(.x = api_start_dates$site,
                                           .y = api_start_dates$DT_round,
                                           ~api_puller(site = .x, 
                                                       start_dt = .y, 
                                                       end_dt = .y + hours(3), # CHANGE TO Sys.time()
                                                       api_token = hv_token,
                                                       dump_dir = here("data", "api"),
                                                       network = "FCW"))
```

*Load in all the raw files*

```{r}
new_data <- munge_api_data(api_path = here("data", "api"),
                           # select which sondes to download data from
                           network = "FCW") %>%
    split(f = list(.$site, .$parameter), sep = "-") %>%
   purrr::keep(~ nrow(.) > 0)
```

*Here, we split up all of our new data into site-parameter combinations (as a list) that we can more easily iterate over. Then, across those lists, we average any observations whose frequency is greater than 15 minutes so that our data set is consistently recorded at 15-minute intervals. (Having data recording at something other than 15 minutes is super rare in  this dataset.) We also preserve the total number of observations within the 15-minute increment used to calculate the mean, as well as the spread (max-min). After these calculations, we use {padr}'s `pad()` function to fill in data gaps at this 15-minute interval. Lastly, we join these data frames with the field notes.*

```{r}
# format and summarize data
# Determine each site and parameter in all_data 
sites <- unique(dplyr::bind_rows(new_data) %>% dplyr::pull(site))
params <- c(
  "Chl-a Fluorescence",
  "Depth",
  "DO",
  "ORP",
  "pH",
  "Specific Conductivity",
  "Temperature",
  "Turbidity")
# 
# # Constructing a df to iterate over each site-parameter combination
site_param_combos <- tidyr::crossing(sites, params) %>% 
  dplyr::mutate(combo = paste0(sites, "-", params)) %>% 
  dplyr::pull(combo) 

# Make a list of the 15-minute summarized data, joined with field notes
new_data_tidied_list <-  new_data[names(new_data) %in% site_param_combos] %>%
  purrr::map(~tidy_api_data(api_data = .,
                            # should be the same interval as what was selected for
                            # all upstream steps
                            summarize_interval = "15 minutes")) %>% 
  # remove NULL from the list (indicating a site-param combo that doesn't actually exist)
  purrr::keep(~ !is.null(.))
```

Combine our new data with the 24-hour period before it, then add field notes:

```{r}
combined_data <- combine_datasets(incoming_data_list = new_data_tidied_list,
                                  historical_data_list = historical_data) %>%
  purrr::map(~add_field_notes(df = ., 
                              notes = all_field_notes)) 
```

#### *Add summary stats*

Here, we are adding in contextual summary statistics that can be used to describe a given
observation's relationship to its neighboring observations. This includes:

-   the previous and next observation and their slopes*
-   the 7-point (each observation and the previous 6) moving median, mean, slope, and
standard deviation
-   the hydrologic "season" in which the observation lands in. Currently we are defining them as...
Winter base flow: Dec, Jan, Feb, Mar, Apr 
Snow melt: May, Jun 
Monsoon: Jul, Aug, Sep 
Fall base flow: Oct, Nov

```{r}
all_data_summary_stats_list <- combined_data %>%
  purrr::map(~ generate_summary_statistics(.)) 
```

## Step 2: Begin flagging the data 

*Add flagging functions for each df in all_data_summary_list*

*Pass the dfs in all_data_summary_stats_list through the flagging functions:*

```{r}
single_sensor_flags <- purrr::map(all_data_summary_stats_list, function(data) {
  data %>%
    # flag field visits
    add_field_flag(df = .) %>%
    # flag instances outside the spec range
    add_spec_flag(df = ., spec_table = yaml::read_yaml("qaqc_files/sensor_spec_thresholds.yml")) %>%
    # flag data outside of seasonal range
    add_seasonal_flag(df = ., threshold_table = read_csv("qaqc_files/seasonal_thresholds.csv")) %>%
    # flag missing data
    add_na_flag(df = .) %>%
    # flag DO noise 
    find_do_noise(df = .) %>%
    # flag repeating values
    add_repeat_flag(df = .) %>%
    # find times when sonde was moved up/down in housing
    add_depth_shift_flag(df = ., level_shift_table =  all_field_notes, post2024 = TRUE) %>%
    # find instances of sensor drift (FDOM, Chl-a, Turbidity only)
    add_drift_flag(df = .) 
})

intersensor_flags <- single_sensor_flags %>%
  dplyr::bind_rows() %>%
  split(f = .$site, sep = "-") %>%
  # flag times when water was below freezing
  purrr::map(~add_frozen_flag(.)) %>%
  # overflagging correction. remove slope violation flag if it occurs concurrently
  # with temp or depth
  purrr::map(~intersensor_check(.)) %>%
    # add sonde burial. If DO is noise is long-term, likely burial:
  purrr::map(~add_burial_flag(.)) %>%
  # flag times when sonde was unsubmerged
  purrr::map(~add_unsubmerged_flag(.)) %>%
  dplyr::bind_rows() %>%
  data.table::data.table() %>%
  # lil' cleanup of flag column contents
  dplyr::mutate(flag = ifelse(flag == "", NA, flag)) %>%
  # transform back to site-parameter dfs
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::discard(~ nrow(.) == 0) %>%
  # Add in KNOWN instances of sensor malfunction
  purrr::map(~add_malfunction_flag(df = ., malfunction_records = sensor_malfunction_notes))
```

Tidy up the flagging information and data frame to save as new "pwqn_data" file containing all PWQN data.

```{r}
# Then go across sites to remove
# seasonal threshold flags that occurred up-/down-stream at the same time
# Lastly, if over 50% of data is flagged in a moving 2-hour window, flag ALL 
# the data in that window
final_flags <- intersensor_flags %>%
  # creates new column, "auto_flag" that reduces overflagging of drastic system-wide
  # WQ changes
  purrr::map(~network_check(df = ., network = "FCW")) %>%
  dplyr::bind_rows() %>%
  tidy_flag_column() %>%
  split(f = list(.$site, .$parameter), sep = "-") %>%
  purrr::map(~add_suspect_flag(.)) %>%
  dplyr::bind_rows() %>%
  # Remove lonely "suspect" flags after auto-cleaning of data (i.e., suspect observations
  # that are totally isolated and no longer linked to any "real" quality flag)
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA,
                                           ifelse(auto_flag == "suspect data" & is.na(lag(auto_flag, 1)) & is.na(lead(auto_flag, 1)), NA, auto_flag))) %>%
  # remove columns we don't need anymore:
  dplyr::select(c("DT_round", "DT_join", "site", "parameter", "mean", "units", "n_obs", "spread", "auto_flag", "mal_flag", "sonde_moved", "historical")) %>%
  dplyr::mutate(auto_flag = ifelse(is.na(auto_flag), NA, ifelse(auto_flag == "", NA, auto_flag))) %>%
  split(f = list(.$site, .$parameter), sep = "-") 
   
final_historical_bind <- final_data_binder(final_flags, historical_data) %>%
  purrr::keep(~ !is.null(.)) %>%
  dplyr::bind_rows() 
```

## Step 3: Save new updated pwqn dataset, move the api data out of the api folder into archive.

```{r}
saveRDS(final_historical_bind, "data/pwqn_data.RDS")

move_api_data(api_dir = "data/api/", archive_dir = "data/api_archive/")
```

